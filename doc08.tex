% PREAMBLE

\documentclass{article}

% PKGS
\usepackage{setspace}
\usepackage{hyperref}
\usepackage{amsfonts}
\usepackage{xcolor}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
    pdftitle={RLSummaryKIT2026},
    pdfpagemode=FullScreen,
    }

\urlstyle{same}

\title{RL Summary EN}
\author{Georg Pernice, ..}
\date{12.02.2026}

\setcounter{tocdepth}{4}

\begin{document}
	\maketitle
	\newpage
	\doublespacing
	\tableofcontents
	\singlespacing
	\newpage
\section {Preword}
Obtain the Latex source code for this document here to compile translate it or create your own version: 
\href{https://github.com/g14p/learninglatex}{https://github.com/g14p/learninglatex}
\section {00 - Orga and Intro}
\section {01 -  Bandits and Explore}
\section {02 - Opt. Dec. Making}
\section {04 - Value Based Meth.}
\section {05 - Policy Gradient}
\section {06 - Offpolicy Actor critic}
\section {07 - Control as Inference  }
\section {08 - Trust Region Methods}
\section {09 - Imitation Learning}
\section {10 - Offline RL}
\section {12 - Motion Primitives and Temp. Extend. Actions}
\section {13 - Trafos}
\section {14 - Diffusion}
\section {14b Diffusion2}
\section {15 - Diverse Behavior and Skill Discovering}
\section {16 - Diffusion RL and Scaling RL}
\subsection{Self Check Questions}
    \begin{itemize}
        \item What is problem when reusing off-policy data more often to update network parameters?
            \begin{itemize}
               \item 
               Reusing off-policy data corresponds to increasing Replay Ratio or Update to Data (UTD) Ratio.
               The fact that train data \textbf{and} targets change over time states a problem, leading to 
               Placity Loss and Primary Bias. (See them explained below)
            \end{itemize}
        \item How to counteract the primacy bias?
            \begin{itemize}
                \item
                \textbf{Primacy Bias} = tendency to overfit initial experiences which damages the
                learn proess regarding other experiences. 
                Resetting the environment in intervals helps especially when running SAC algorithm 
                in 'humanoid run' environment.
            \end{itemize}
        \item What kinda regularization exist to  make RL scalable?
            \begin{itemize}
                \item Regularization in ML refers to prevention of overfitting by punishing model
                complexity with additional loss term. (Ideas like Dropout, Punishing model size, ..)
                \item The BRO ( Bigger Regularized Optimistic) algorithm uses 
                    \begin{itemize}
                        \item Weight Decay
                        \item Layer Norm
                        \item ..
                    \end{itemize}
                    It scales the Network by multiple blocks of
                    \begin{itemize}
                        \item Dense Layer
                        \item Layer Norm
                        \item ..
                    \end{itemize}
                \item In general in RL Neuron Resetting seems a valid regularization, used in the 'ReDo'
                (Recycling Dormant Neurons) algorithm.
                \item \textbf{Batch Normalization} acc. to Summary slide seems to be as well a regularization 
                technique in RL
            \end{itemize}
        \item What is the downside to increasing the number of updates?
            \begin{itemize}
                \item Updating more often even does harm to the network return. As the targets will 
                change updating too much on them doesnt make lots of sense. $\rightarrow$ 
                Placicity Loss(lose ability to learn from new XP)  and Primary Bias (overfit initial XP) 
                occur!
                This question seems very similar to the first one imho.
            \end{itemize}

    \end{itemize}
\section{Exercises with Math}
Exercise 2: Policy differentiation
 Compute the gradient of  $log p_\theta (tau)$ , where
$log p_\theta (tau)$ : trajectory distribution induced by params theta
theta     : policy parameters

\subsection{Multivariate Policy Exercise 3}
Consider multi-variate policy distribution 
$\pi(a|s) = \frac{1}{\sqrt{(2\pi)^{d_{e}} |{\sum}|}}$ $exp \left \{-\frac{1}{2} (a - \mu(s))^T \textstyle \sum ^{-1} (a - \mu(s)) \right \}$ 
where $ a \in \mathbb{R}^{d_{e}}$,
$ s \in \mathbb{R}^{d_e}$ 
and we consider an isotropic covariance i.e.
$\textstyle \sum = \sigma^2\textbf{I} $ 
where $\textbf{I} \in \mathbb{R}^{d_e \times d_e}$ 
and $\sigma \in \mathbb{R}^{+}$ 
\\
\\ 
\textbf{Derive $\nabla_\sigma$ $log$ $\pi (a|s)$  }
\textbf{Solution Approach:}
\\
$$
    \nabla_{\sigma} \textstyle{log} \pi( a | s) = 
  \nabla_{\sigma} \textstyle{log} \left \{
    \frac{1}{\sqrt{(2\pi)^{d_{e}} |{\sum}|}} 
    exp \left \{ - \frac{1}{2} 
    (a - \mu(s))^T \textstyle \sum ^{-1} (a - \mu(s)) \right \}
  \right \}
$$
Split the logs the exp dies because of right log. 
Because we get a sum of two logs we can split the gradient as well.
$$
    \nabla_{\sigma} \textstyle{log} \pi( a | s) = \color{red} 
  \nabla_{\sigma} \textstyle{log} \left \{
    \frac{1}{\sqrt{(2\pi)^{d_{e}} \color{green}|{\sum}| \color{blue}}} 
    \right \}\color{black} 
    + \color{blue}\nabla_{\sigma}  
    \left \{ - \frac{1}{2} 
    (a - \mu(s))^T \color{green}\textstyle \sum ^{-1}\color{blue} (a - \mu(s)) \right \}
$$

With the two identities 
$\color{green}\textstyle{\sum^{-1}}=\frac{1}{\sigma^2}\textbf{I}$
and
$\color{green}\textstyle{|\sum|} = (\sigma^2)^{d_e} $
for the covariance we arrive at:


$$
    \nabla_{\sigma} \textstyle{log} \pi( a | s) = \color{red} 
  \nabla_{\sigma} \textstyle{log} \left \{
    \frac{1}{\sqrt{(2\pi)^{d_{e}} \color{green}|(\sigma^2)^{d_e}| \color{blue}}} 
    \right \}\color{black} 
    + \color{blue}\nabla_{\sigma}  
    \left \{ - \frac{1}{2}  \color{green}\textstyle \frac{1}{\sigma^2}\color{blue} 
    (a - \mu(s))^T \color{green}\textstyle \textbf{I}\color{blue} (a - \mu(s)) \right \}
$$


\end{document}




